{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation)\n",
    "- RAG is a technique using or retrieving data from private or real-time sources to increase, expand the capabilities of the LLMs.\n",
    "![image](./images/Architecture.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "## Retrieval\n",
    "\n",
    "![image](./images/data_connection-95ff2033a8faa5f3ba41376c0f6dd32a.jpg)\n",
    "\n",
    "1. From the source file, load the data by using text loader (UnstructuredFileLoader).\n",
    "2. Transform it by splitting the data, because it is better for LLM to search multiple smaller documents rather than single big document. \n",
    "3. Embed the data. Embedding menas a vector representation of the meaning behind the text, documents (OpenAIEmbeddings).\n",
    "4. Use CacheBackedEmbedding to cache the embeddings because it is not free.\n",
    "4. Store the number (data).\n",
    "5. Perform a search by using vectorestore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 21:39:41.417 Python[17572:719860] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Document Loaders\n",
    "    - Loader is a piece of code extract the data from a source and brings it to Langchain.\n",
    "    - https://python.langchain.com/docs/integrations/document_loaders/unstructured_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "# from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "# loader = TextLoader(\"./files/chapter_one.txt\")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.pdf\")\n",
    "\n",
    "# loader.load()\n",
    "len(loader.load())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will split the document. The return value of loader.load() is a list, and the whole chapter is only one document. It is better and efficient to split the document to store, embed, and give it to language model.\n",
    "\n",
    "RecursiveCharacterTextSplitter will separate the file for every sentence or paragraph ending to keep the semantic meaning of sentences.\n",
    "\n",
    "## Tiktoken\n",
    "- token doesn't mean just a letter, it could be a word, a text, or a chunk of text. \n",
    "- To see the difference between token and characters, refer to OpenAI tokenizer : https://platform.openai.com/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=100,\n",
    "    separator=\"\\n\",\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "\n",
    "loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "len(loader.load_and_split(text_splitter=splitter))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectors\n",
    "Below cell is just an example of how each word in the list converted to vectors (numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1536\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedder = OpenAIEmbeddings()\n",
    "\n",
    "# this is the vector with all dimesions for the word \"hi\"\n",
    "hi = embedder.embed_query(\"Hi\")\n",
    "len(hi) #total 1536 dimensions only for the word \"hi\"\n",
    "\n",
    "vector = embedder.embed_documents(\n",
    "    [\n",
    "    \"hi\",\n",
    "    \"how\",\n",
    "    \"are\",\n",
    "    \"you longer sentences\",\n",
    "    ]\n",
    ")\n",
    "print(len(vector), len(vector[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Vector stores is sort of databases.\n",
    " 1) Create vectors  \n",
    " 2) Cache those vectors  \n",
    " 3) Put those vectors inside of the Vector store \n",
    " 4) Perform searches to find relevant docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without cache, if we re-run the entire cell, this would cost more money. So it is better to save it in cache. \n",
    "Cache these embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "# Load \n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "\n",
    "# Transform \n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "# Embed \n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# When we embed the file, first, we check if those embeddings already exist in our cache.\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "\n",
    "# If not, call vectorestore chroma\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start search in vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectorstore.similarity_search(\"where does Mr.Jones live\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The new context does not provide any information about Major's age. Therefore, the original answer of Major's age being twelve years old remains the same.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"How old is Major?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Stuff LCEL Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 322, which is longer than the specified 300\n",
      "Created a chunk of size 302, which is longer than the specified 300\n",
      "Created a chunk of size 308, which is longer than the specified 300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Old Major sang a song called \"Beasts of England.\" The tune was described as a stirring one, resembling a combination of \"Clementine\" and \"La Cucaracha.\" The lyrics of the song spoke of a golden future time when animals would be free from the tyranny of Man. It expressed the hope that one day, animals would overthrow humans and live in a world where they would have abundance and freedom.')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "# Load \n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "\n",
    "# Transform \n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "# Embed \n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# When we embed the file, first, we check if those embeddings already exist in our cache.\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "\n",
    "# If not, call vectorestore chroma\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", '''You are a helpful assistant.Answer questions using only \n",
    "     the following context. If you don't know the answer just say you don't know,\n",
    "     don't make it up:\\n\\n{context}'''),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Retriever is a very important component of the chain.\n",
    "# Langchain will take query, and call retriever(\"Describe the sing that old Major sang.\")\n",
    "# Then we get the list of documents [Doc].\n",
    "# Then, RunnablePassthrough allow us to pass the input through it.\n",
    "# Hence, a list of Documents and question has passed onto the prompt.\n",
    "chain = {\"context\":retriever, \"question\": RunnablePassthrough()} | prompt | llm\n",
    "\n",
    "chain.invoke(\"Describe the sing that old Major sang.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing MapReduce Chain using LCCE\n",
    "This is a simplified version to understand MapReduce Chain.\n",
    "- List of Documents\n",
    "- For every documents in the list of documents, we are going to create a prompt, and give it to LLM.\n",
    "Prompt: Read this document and see if it has a relevant info to answer the questions that the user request.\n",
    "Then we get a response from LLM, then from every response in the list of llms responses, we create a whole new document for that, and put them all together.\n",
    "And this final document will be given to the prompt, and to the LLMs, to answer the initial question. \n",
    "\n",
    "To recap, if we ask a question (\"Describe the sing that old Major sang.\"), this wil pass onto the retriever, and the retriever will get us a list of documents that are relevant to the question. For every document in that list of document, we create a prompt, give that prompt to LLM. For example, if there are 5 documents, we are going to ask LLM 5 times, then we receive 5 responses from LLM. We gather those responses, and put them all together in one long document. Then, with that, we talk to LLM put that final document to prompt. Prompt is basiaclly saying \"These are all relevant information for query. \n",
    "\n",
    "This MapReduce Chain works better when there are more than thousands of documents that retriever returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 322, which is longer than the specified 300\n",
      "Created a chunk of size 302, which is longer than the specified 300\n",
      "Created a chunk of size 308, which is longer than the specified 300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The song that Old Major sang is called \"Beasts of England.\"')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# list of docs\n",
    "\n",
    "# for doc in list of docs | prompt | llm\n",
    "\n",
    "# for response in list of llms response | put them all together\n",
    "\n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "          \"system\",\n",
    "          \"\"\"\n",
    "          Use the following portion of a long document to see if any of the\n",
    "          text is relevant to answer the question. Return any relevant text\n",
    "          verbatim.\n",
    "          -------\n",
    "          {context}\n",
    "          \"\"\" , \n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "'''\n",
    "def map_docs(inputs):\n",
    " #   print(inputs)\n",
    "    documents = inputs['documents']\n",
    "    question = inputs['question']\n",
    "    results = []\n",
    "    # for each of document in the input documents, run map_doc_chain for each one of them.\n",
    "    # save that response, and make a list of responses, and turn them into string in a long document.\n",
    "    for document in documents:\n",
    "        result = map_doc_chain.invoke({\n",
    "            \"context\": document.page_content,\n",
    "            \"question\": question\n",
    "        }).content\n",
    "        results.append(result)\n",
    "    results = \"\\n\\n\".join(results)\n",
    "# print(results)\n",
    "    return results\n",
    "'''\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "    return \"\\n\\n\".join(\n",
    "        map_doc_chain.invoke(\n",
    "            {\"context\": doc.page_content, \"question\": question}\n",
    "        ).content\n",
    "        for doc in documents\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# RunnableLambda class allows you to call a function inside of any chain.\n",
    "# retriever has a input type of string, and give us documents.\n",
    "map_chain = {\"documents\": retriever, \"question\": RunnablePassthrough()} | RunnableLambda(map_docs)\n",
    "\n",
    "# final doc | prompt | llm\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\"system\",\n",
    "     \"\"\"Given the following extracted parts of a long document and a\n",
    "     question, create a final answer.\n",
    "     If you don't know the answer, just say that you don't know. Don't try\n",
    "     to make up an answer..\n",
    "     ------\n",
    "     {context}\n",
    "     \"\"\",\n",
    "     ),\n",
    "    (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# process each doucment in the list, and see if it contains relevant info to answer the question\n",
    "\n",
    "\n",
    "chain =  {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "\n",
    "chain.invoke(\"Name a song that Old Major sang.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
