{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ChatPromptTemplate is to create a template from messages.\n",
    "- prompt template is to create a template from string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.llms.openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "# streaming means we see the response of the model as it happens instead of just waiting till the end.\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()]) # -> gpt-3.5 turbo model\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distance between Korea and Thailand is approximately 3,000 kilometers (1,864 miles)."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The distance between Korea and Thailand is approximately 3,000 kilometers (1,864 miles).'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = PromptTemplate.from_template(\"what is the distance between {country_a} and {country_b}\")\n",
    "#llm = OpenAI() -> text-davinci model\n",
    "    # temperature determines how creative or random the model is.\n",
    "\n",
    "prompt = template.format(country_a = \"Korea\", country_b = \"Thailand\")\n",
    "chat.predict(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict message using gpt-3.5 turbo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao! Il mio nome è Paolo. La distanza tra il Messico e la Thailandia è di circa 16.000 chilometri."
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='Ciao! Il mio nome è Paolo. La distanza tra il Messico e la Thailandia è di circa 16.000 chilometri.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "message = [\n",
    "    SystemMessage(content=\"You are a geography expert. And you only reply in Italian.\"),\n",
    "    AIMessage(content=\"Ciao, mi ciamo Paulo.\"),\n",
    "    HumanMessage(content=\"What is the distance between Mexico and Thailand? Also, What is your name? \")\n",
    "]\n",
    "\n",
    "chat.predict_messages(message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we will use ChatPromptTemplate to make it simpler.\n",
    "- template -> format -> predict method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distance between Korea and Thailand varies depending on the specific locations. The approximate distance between Seoul, South Korea, and Bangkok, Thailand, is around 4,500 kilometers (2,800 miles). However, please note that this is a straight-line distance and actual travel distances may differ.\n",
      "\n",
      "As for my name, I am an AI language model developed by OpenAI, and I don't have a personal name. You can simply refer to me as \"Assistant.\" How can I assist you further?"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='The distance between Korea and Thailand varies depending on the specific locations. The approximate distance between Seoul, South Korea, and Bangkok, Thailand, is around 4,500 kilometers (2,800 miles). However, please note that this is a straight-line distance and actual travel distances may differ.\\n\\nAs for my name, I am an AI language model developed by OpenAI, and I don\\'t have a personal name. You can simply refer to me as \"Assistant.\" How can I assist you further?')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a geography expert. And you only reply in {language}.\"),\n",
    "        (\"ai\", \"My name is {name}.\"),\n",
    "        (\"human\", \"What is the distance between {country_a} and {country_b}? Also, What is your name?\")\n",
    "    ]\n",
    ")\n",
    " \n",
    "prompt = template.format_messages(\n",
    "    language = \"English\",\n",
    "    name = \"Joong Ho\",\n",
    "    country_a = \"Korea\",\n",
    "    country_b = \"Thailand\"\n",
    ")\n",
    "\n",
    "chat.predict_messages(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OutputParser and LangChain Expression Language(LCEL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Need Output Parser to transform the response from LLM\n",
    "- It could be List, Dictionary, or database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser \n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    \n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip,items))\n",
    "\n",
    "# p = CommaOutputParser()\n",
    "# p.parse(\"Hello, How, are,you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red, orange, yellow, green, blue, indigo, violet, black, white, gray"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['red',\n",
       " 'orange',\n",
       " 'yellow',\n",
       " 'green',\n",
       " 'blue',\n",
       " 'indigo',\n",
       " 'violet',\n",
       " 'black',\n",
       " 'white',\n",
       " 'gray']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a list generating machine. Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase. Do NOT reply iwth anything else.\",\n",
    "     ),\n",
    "    (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=\"10\",\n",
    "    question=\"What are the colors?\"\n",
    "    )\n",
    "\n",
    "#chat.predict_messages(prompt)\n",
    "result = chat.predict_messages(prompt)\n",
    "\n",
    "p = CommaOutputParser()\n",
    "p.parse(result.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make above cell very simple and short by creating \"chain\".\n",
    "- Need Template, Output Parser, and Chat Model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pikachu, charizard, bulbasaur, squirtle, jigglypuff"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['pikachu', 'charizard', 'bulbasaur', 'squirtle', 'jigglypuff']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a list generating machine. Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase. Do NOT reply iwth anything else.\",\n",
    "     ),\n",
    "    (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# this chain combines template, language model and Output Parser\n",
    "chain = template | chat | CommaOutputParser()\n",
    "\n",
    "# this is example. you can also chain among chains\n",
    "# chain_one = template | chat | CommaOutoputParser()\n",
    "# chain_two = template2 | chat | OutputParser()\n",
    "# first, combine chain_one, and you can use output of chain_one to use it with chain_two\n",
    "# all = chain_one | chain_two| output\n",
    "\n",
    "chain.invoke({\n",
    "    \"max_items\":5,\n",
    "    \"question\":\"What are Pokemons?\"\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining Chains\n",
    "- https://python.langchain.com/docs/expression_language/interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy example\n",
    "chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", '''You are a world-class international chef.You create easy to follow \n",
    "     recipies for any type of cuisine with easy to find ingredients.'''),\n",
    "    (\"human\", \"I want to cook {cuisine} food\"),   \n",
    "])\n",
    "\n",
    "# create chef chain then send chat to LLM\n",
    "# we are going to receive recipe from a chef, and we will adapt it to use vegetarian ingredients.\n",
    "chef_chain = chef_prompt | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"To make a vegetarian version of Chicken Tikka Masala, you can replace the chicken with a plant-based protein such as tofu or tempeh. Here's how you can modify the recipe:\\n\\nIngredients:\\n- 500g tofu or tempeh, cut into bite-sized pieces\\n- 1 cup plain yogurt (use dairy-free yogurt for a vegan version)\\n- 2 tablespoons lemon juice\\n- 2 tablespoons vegetable oil\\n- 1 large onion, finely chopped\\n- 3 cloves of garlic, minced\\n- 1 tablespoon ginger, grated\\n- 2 teaspoons ground cumin\\n- 2 teaspoons ground coriander\\n- 1 teaspoon turmeric powder\\n- 1 teaspoon paprika\\n- 1 teaspoon garam masala\\n- 1 cup tomato puree\\n- 1 cup coconut cream (or dairy-free heavy cream)\\n- Salt, to taste\\n- Fresh cilantro, chopped (for garnish)\\n\\nInstructions:\\n1. In a bowl, combine the yogurt, lemon juice, 1 teaspoon of ground cumin, 1 teaspoon of ground coriander, turmeric powder, paprika, and salt. Mix well.\\n2. Add the tofu or tempeh pieces to the marinade and coat them evenly. Let it marinate for at least 30 minutes, or refrigerate overnight for better flavor.\\n3. Heat the vegetable oil in a large skillet over medium heat. Add the chopped onion and sauté until golden brown.\\n4. Add the minced garlic and grated ginger to the skillet and cook for another minute.\\n5. Add the remaining ground cumin, ground coriander, and garam masala to the skillet. Stir well to combine the spices with the onion mixture.\\n6. Add the tomato puree and cook for a few minutes until the oil starts to separate from the mixture.\\n7. Add the marinated tofu or tempeh pieces to the skillet, along with any remaining marinade. Stir well to coat the protein with the sauce.\\n8. Reduce the heat to low, cover the skillet, and let it simmer for about 15-20 minutes, or until the tofu or tempeh is heated through.\\n9. Stir in the coconut cream (or dairy-free heavy cream) and simmer for an additional 5 minutes.\\n10. Taste and adjust the seasoning with salt if needed.\\n11. Garnish with freshly chopped cilantro before serving.\\n12. Serve the Vegetarian Tikka Masala with steamed rice or naan bread.\\n\\nEnjoy your homemade vegetarian Indian feast!\")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", '''You are a vegetarian chef specialized on making traditional recipies\n",
    "     vegetarian. You find alterative ingredients and explain their preparation. You\n",
    "     don't radically modify the recipe. If there is no alternative for a food just \n",
    "     say you don't know how to replace it.'''),\n",
    "    (\"human\", \"{recipe}\")\n",
    "])\n",
    "\n",
    "veg_chain = veg_chef_prompt | chat\n",
    "\n",
    "# final_chain = chef_chain | veg_chain\n",
    "# final_chain.invoke({\n",
    "#    \"cuisine\": \"indian\"  \n",
    "# })\n",
    "# veg_chain.invoke({\n",
    "#     \"recipe\": \"chatmodel\"\n",
    "# })\n",
    "\n",
    "\n",
    "# Instead of calling two invoke function, input called recipe of the veg_chain come from chef_chain\n",
    "# putting context obejct for the chain\n",
    "# store the result of chef_chain on a key called \"recipe\"\n",
    "# give this recipe key to the veg_chain\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chain\n",
    "final_chain.invoke({\"cuisine\": \"indian\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
