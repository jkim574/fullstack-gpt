{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory in Langchain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to add memory in chatbot so that chatbot can remember your previous chat. The Default OpenAI API without langchain doesn't support memory. Hence, it is our job to add the history to the prompt that LLM would get. \n",
    "\n",
    "Here are a list of memories and how each of them are different, and implement each type of memory into the chain.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Buffer\n",
    "- Very simple, and saves the whole memory.\n",
    "- This is useful when using text completion, prediction.\n",
    "- However, it is not efficient when working with chat model. The longer conversation, the memory also grows which cost a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi'), AIMessage(content='How are you?')]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# initialize memory\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Buffer Window\n",
    "- it remembers the most recent messages by configuring the number of messages\n",
    "- can keep a memory upto a certain size\n",
    "- However, the chatbot will only remember the most recent conversation, not the entire chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    # how many interactions you want\n",
    "    k=4\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\":output})\n",
    "    \n",
    "add_message(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(2, 2)\n",
    "add_message(3, 3)\n",
    "add_message(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='1'),\n",
       "  AIMessage(content='1'),\n",
       "  HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4')]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4'),\n",
       "  HumanMessage(content='5'),\n",
       "  AIMessage(content='5')]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as the buffer grows to 5, the number one message is gone.\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Summary Memory\n",
    "- it summarize the conversation.\n",
    "- ConversationSummaryMemory will require more tokens at the beginning.\n",
    "- but as the conversation becomes longer, making a summary of them is better, which reduces the amount of token count.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi, I;m Joong, I live in California\", \"Wow that is cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"California is so pretty\", \"I wish I could visit there.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human introduces themselves as Joong and mentions that they live in California. The AI responds by expressing admiration for this information and expresses a desire to visit California because it is so pretty.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Summary Buffer Memory\n",
    "- A mixture of Conversation Summary Memory and Conversation Buffer Memory\n",
    "- keeps track of the most recent interactions with messages\n",
    "- the moment it reaches the limit, instead of forgetting what happened previously, it will summarize the older messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=70,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi, I;m Joong, I live in California\", \"Wow that is cool!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, I;m Joong, I live in California'),\n",
       "  AIMessage(content='Wow that is cool!')]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"California is so pretty\", \"I wish I could visit there.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, I;m Joong, I live in California'),\n",
       "  AIMessage(content='Wow that is cool!'),\n",
       "  HumanMessage(content='California is so pretty'),\n",
       "  AIMessage(content='I wish I could visit there.')]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"How far is California from Wisconsin?\", \"I don't know!.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human introduces themselves as Joong and mentions that they live in California.'),\n",
       "  AIMessage(content='Wow that is cool!'),\n",
       "  HumanMessage(content='California is so pretty'),\n",
       "  AIMessage(content='I wish I could visit there.'),\n",
       "  HumanMessage(content='How far is California from Wisconsin?'),\n",
       "  AIMessage(content=\"I don't know!.\")]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"Then how far is Wisconsin from Arizona?\", \"That's pretty far!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "once it hits the limit, it produces a summarization of the older system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human introduces themselves as Joong and mentions that they live in California. The AI responds with enthusiasm, saying \"Wow, that is cool!\"'),\n",
       "  HumanMessage(content='California is so pretty'),\n",
       "  AIMessage(content='I wish I could visit there.'),\n",
       "  HumanMessage(content='How far is California from Wisconsin?'),\n",
       "  AIMessage(content=\"I don't know!.\"),\n",
       "  HumanMessage(content='Then how far is Wisconsin from Arizona?'),\n",
       "  AIMessage(content=\"That's pretty far!\")]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Knowledge Graph Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "add_message(\"Hi, I;m Joong, I live in California\", \"Wow that is cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Joong: Joong lives in California.')]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"who is Joong?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"Joong likes python.\", \"Wow that is cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Joong: Joong lives in California. Joong likes python.')]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"inputs\": \"What does Joong like\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is hard to imagine how chatbot is going to use this if the user has to inform this manually. It is annoying to have \".save_context\" and \".load_memory_variables\". \n",
    "There is a chain automatically working with memory without having to manually inform everything.\n",
    "\n",
    "There are many memory integrations with various databases. https://python.langchain.com/docs/integrations/memory\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to plug memory into chain\n",
    "- Off-the_shelf LLM Chain : it automatically receives a response from LLM and updates the memory, but it is still our job to put that history of the memory on the prompt.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will have a look at auto-complete a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory1 = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=80,\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory1,\n",
    "    prompt=PromptTemplate.from_template(\"{question}\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Joong! How can I assist you today?'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"My name is Joong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's great! California is a beautiful state known for its diverse landscapes, including stunning beaches, towering mountains, and vast deserts. It is also home to vibrant cities like Los Angeles, San Francisco, and San Diego. California offers a wide range of activities and attractions, from exploring national parks like Yosemite and Joshua Tree to enjoying the entertainment industry in Hollywood. The state is also known for its progressive culture, technological advancements, and thriving economy.\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in California.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation.\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it does not remember the previous chat. We will start debugging by passing verbose property in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mMy name is Joong\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Joong! How can I assist you today?'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory2 = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=80,\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory2,\n",
    "    prompt=PromptTemplate.from_template(\"{question}\"),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain2.predict(question=\"My name is Joong\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by passing verbose property, we can see prompt log of the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI live in California.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great! California is a beautiful state known for its diverse landscapes, including stunning beaches, majestic mountains, and vast deserts. It is also home to vibrant cities like Los Angeles, San Francisco, and San Diego. California offers a wide range of activities and attractions, from exploring national parks like Yosemite and Joshua Tree to enjoying the entertainment industry in Hollywood. The state is also known for its progressive culture, technological advancements, and thriving economy.\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2.predict(question=\"I live in California.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWhat is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2.predict(question=\"What is my name?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the history of conversation is not being added to the prompt. Hence, we need to add the history of the conversation into the prompt by adding memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human introduces themselves as Joong. The AI greets Joong and asks how it can assist. Joong mentions that they live in California. The AI responds by describing California's diverse landscapes, vibrant cities, and popular tourist attractions. The AI also offers to provide information or discuss any specific topics related to California.\\nHuman: What is my name?\\nAI: I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation.\"}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history and memory are being updated with inputs and outputs of human and LLMs. The LLM is not given the history of conversation, which should go into the prompt, which only gives LLM the question.\n",
    "All we have to do is making a space for the memory in our prompt template. Then, tell Conversation Summary Buffer Memory (or, any Memory Class you are using) where to put its history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of calling \"memory.load.variables()\", call memory_key property.\n",
    "memory3 = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "# This is a string-based Template.\n",
    "template = \"\"\" \n",
    "\n",
    "    You are a helpful AI talking to a human.\n",
    "    \n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "chain3 = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory3,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "    You are a helpful AI talking to a human.\n",
      "    \n",
      "    \n",
      "    Human:My name is Joong\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Joong! How can I assist you today?'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain3.predict(question=\"My name is Joong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "    You are a helpful AI talking to a human.\n",
      "    \n",
      "    Human: My name is Joong\n",
      "AI: Hello Joong! How can I assist you today?\n",
      "    Human:I live in California.\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great! California is a beautiful state. How can I assist you today, Joong?\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain3.predict(question=\"I live in California.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "    You are a helpful AI talking to a human.\n",
      "    \n",
      "    Human: My name is Joong\n",
      "AI: Hello Joong! How can I assist you today?\n",
      "Human: I live in California.\n",
      "AI: That's great! California is a beautiful state. How can I assist you today, Joong?\n",
      "    Human:What is my name?\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Joong.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain3.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "    You are a helpful AI talking to a human.\n",
      "    \n",
      "    Human: My name is Joong\n",
      "AI: Hello Joong! How can I assist you today?\n",
      "Human: I live in California.\n",
      "AI: That's great! California is a beautiful state. How can I assist you today, Joong?\n",
      "Human: What is my name?\n",
      "AI: Your name is Joong.\n",
      "Human: What is my name?\n",
      "AI: Your name is Joong.\n",
      "    Human:My favorite language is python.\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great to hear! Python is a popular programming language known for its simplicity and readability. How can I assist you with Python today, Joong?\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain3.predict(question=\"My favorite language is python.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding memory to a message-based conversation is easy. First, you need to memory classes can output memory in two ways.\n",
    "- string\n",
    "- message format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "memory4 = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True # Don't turn them into string, but chat message format.\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you are a helpful AI talking to a human\"),\n",
    "    # since we don't know how long the conversation would be, how do we decide make space for that memory?\n",
    "    # Hence, import MessagesPlaceholder \n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), # conversationsummarybuffermemory will get messages from its history, and is going to fill messageplaceholder with all these message.\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain4 = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory4,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: you are a helpful AI talking to a human\n",
      "Human: My name is Joong\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Joong! How can I assist you today?'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain4.predict(question=\"My name is Joong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: you are a helpful AI talking to a human\n",
      "Human: My name is Joong\n",
      "AI: Hello Joong! How can I assist you today?\n",
      "Human: I live in California.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great, Joong! California is a beautiful state with a lot to offer. Is there anything specific you would like to know or discuss about living in California?\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain4.predict(question=\"I live in California.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: you are a helpful AI talking to a human\n",
      "Human: My name is Joong\n",
      "AI: Hello Joong! How can I assist you today?\n",
      "Human: I live in California.\n",
      "AI: That's great, Joong! California is a beautiful state with a lot to offer. Is there anything specific you would like to know or discuss about living in California?\n",
      "Human: What is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Joong, as you mentioned earlier. Is there anything else you would like to know or discuss?'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain4.predict(question=\"What is my name?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain Expressions Language\n",
    "- we can also do this job by ourselves by using Langchain Expressinos Language.\n",
    "- Have to fill up settings manually, but it is quite useful when it comes to customizing since the configuration is exposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The problem of this approach is everytime when we invoke the chain, we also have to add the chat history.\n",
    "# chain = prompt | llm\n",
    "\n",
    "# chain.invoke({\n",
    "#     \"chat_history\": memory.load_memory_variables({})[\"chat_history\"],\n",
    "#     \"question\": \"My name is Joong.\"\n",
    "# })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnablePassthrough allows you to run a function before the prompt is formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    # memory_key=\"chat_history\", (don't need this anymore since the memory key is history by default.)\n",
    "    return_messages=True, # Don't turn them into string, but chat message format.\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you are a helpful AI talking to a human\"),\n",
    "    # since we don't know how long the conversation would be, how do we decide make space for that memory?\n",
    "    # Hence, import MessagesPlaceholder \n",
    "    MessagesPlaceholder(variable_name=\"history\"), # conversationsummarybuffermemory will get messages from its history, and is going to fill messageplaceholder with all these message.\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "# Langchain will first call load_memory function\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "# after calling the function, it will put it inside of chat_history that prompt needs.\n",
    "# the output of running load_memory should be given to chat_history property\n",
    "# and that combined with the input of the user is going to be given to the prompt.\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "# chain = prompt | llm\n",
    "\n",
    "# Since we are doing memory management manually, we have to save the interaction between the user and the machine into the memory.\n",
    "# invoke_chain function invoke the chain, save that interaction in the memory.\n",
    "def invoke_chain(question):\n",
    "    # The dictionary in this invoke will be the input for the first item on the chain.\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context( # save_context adds input and output to the memory.\n",
    "        {\"input\": question}, \n",
    "        {\"output\": result.content}\n",
    "    ),\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello Joong! How can I assist you today?'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"My name is Joong.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Your name is Joong, as you mentioned earlier. Is there anything specific you would like assistance with, Joong?'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
